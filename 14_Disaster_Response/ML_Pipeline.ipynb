{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anubhav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anubhav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    \"\"\"\n",
    "    This function loads data from given database path \n",
    "    and returns a dataframe\n",
    "    Input:\n",
    "        database_filepath: database file path\n",
    "    Output:\n",
    "        X: traing message list\n",
    "        Y: training target\n",
    "        category names  \n",
    "    \"\"\"\n",
    "    # load data from database\n",
    "    engine = create_engine('sqlite:///'+ database_filepath)\n",
    "    df = pd.read_sql_table('messages',engine)\n",
    "    \n",
    "    # define features and target\n",
    "    X = df.message\n",
    "    y = df.iloc[:,4:]\n",
    "    category_names = list(df.columns[4:])\n",
    "    \n",
    "    return X, y, category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenization function to process the text data to normalize, lemmatize, and tokenize text. \n",
    "    Input: Text data\n",
    "    Output: List of clean tokens \n",
    "    \"\"\"\n",
    "     # remove punctations\n",
    "    #text =  ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    #tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        # lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(token).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Build Machine learning pipleine using Adaboost Classifier\n",
    "    Input:\n",
    "       None\n",
    "    Output: \n",
    "        clf: gridSearch Model\n",
    "    \"\"\"\n",
    "    ada_pipeline =  Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier((AdaBoostClassifier())))\n",
    "    ])\n",
    "    # grid search parameters\n",
    "    parameters = {\n",
    "    'tfidf__norm':['l2','l1'],\n",
    "    'vect__stop_words': ['english',None],\n",
    "    'clf__estimator__learning_rate' :[0.1, 0.5, 1, 2],\n",
    "    'clf__estimator__n_estimators' : [50, 60, 70],\n",
    "    }\n",
    "    #create grid search object\n",
    "    clf_grid_model = GridSearchCV(ada_pipeline, parameters)\n",
    "    return clf_grid_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    \"\"\"\n",
    "    Prints the classification report for the given model and test data\n",
    "    Input:\n",
    "        model: trained model\n",
    "        X_test: test data for the predication \n",
    "        Y_test: true test labels for the X_test data\n",
    "    Output:\n",
    "        None \n",
    "    \"\"\"\n",
    "    # predict \n",
    "    y_pred = model.predict(X_test)\n",
    "    # print the metrics\n",
    "    for i, col in enumerate(category_names):\n",
    "        print('{} category metrics: '.format(col))\n",
    "        print(classification_report(Y_test.iloc[:,i], y_pred[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_filepath):\n",
    "    \"\"\"\n",
    "    This method is used to export a model as a pickle file\n",
    "    Input:\n",
    "        model: trained model \n",
    "        model_filepath: location to store the model\n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    joblib.dump(model, model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(database_filepath, model_filepath):\n",
    "    print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "    X, Y, category_names = load_data(database_filepath)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    print('Building model...')\n",
    "    model = build_model()\n",
    "\n",
    "    print('Training model...')\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    print('Evaluating model...')\n",
    "    evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "    print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "    save_model(model, model_filepath)\n",
    "\n",
    "    print('Trained model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "    DATABASE: data/DisasterResponse.db\n",
      "Building model...\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "F:\\DO_NOT_TOUCH\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "database_filepath = 'data/DisasterResponse.db'\n",
    "model_filepath = 'data/classifier.pkl'\n",
    "main(database_filepath,model_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
